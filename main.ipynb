{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5060a3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.7/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/yc-user/.env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "%run -n main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f1cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = read_lines('.secret')\n",
    "# env = dict(parse_dotenv(lines))\n",
    "# openai.api_key = env['OPENAI_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91801c88",
   "metadata": {},
   "source": [
    "# tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = pd.read_excel('tasks/user_oriented_annot.xlsx')\n",
    "# table = table[~table.done.isnull()]\n",
    "# table = table.where(pd.notnull(table), None)\n",
    "\n",
    "# view = table[['id', 'orig_instruction', 'orig_input']]\n",
    "# view = view.rename(columns={\n",
    "#     'orig_instruction': 'instruction',\n",
    "#     'orig_input': 'input'\n",
    "# })\n",
    "# items = view.to_dict(orient='records')\n",
    "# lines = format_jsonl(items)\n",
    "# write_lines('tasks/user_oriented_en.jsonl', lines)\n",
    "\n",
    "# view = table[['id', 'instruction', 'input']]\n",
    "# items = view.to_dict(orient='records')\n",
    "# lines = format_jsonl(items)\n",
    "# write_lines('tasks/user_oriented_ru.jsonl', lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = pd.read_excel('tasks/vicuna_question_annot.xlsx')\n",
    "\n",
    "# view = table[['id', 'category', 'instruction']]\n",
    "# items = view.to_dict(orient='records')\n",
    "# lines = format_jsonl(items)\n",
    "# write_lines('tasks/vicuna_question_ru.jsonl', lines)\n",
    "\n",
    "# view = table[['id', 'category', 'orig_instruction']]\n",
    "# view = view.rename(columns={'orig_instruction': 'instruction'})\n",
    "# items = view.to_dict(orient='records')\n",
    "# lines = format_jsonl(items)\n",
    "# write_lines('tasks/vicuna_question_en.jsonl', lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277a70a",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75304a05",
   "metadata": {},
   "source": [
    "## gusev_ru_alpaca_7b_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822527ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = 'IlyaGusev/llama_7b_ru_turbo_alpaca_lora'\n",
    "config = PeftConfig.from_pretrained(model_name)\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "\n",
    "    # Overriding torch_dtype=None with `torch_dtype=torch.float16\n",
    "    torch_dtype=torch.float16,\n",
    "    \n",
    "    # A device map needs to be passed to run convert models into mixed-int8\n",
    "    device_map='auto'\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d3866",
   "metadata": {},
   "source": [
    "## gusev_ru_alpaca_16b_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd03fbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d2a90b471440a7a7037763751012c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'IlyaGusev/llama_13b_ru_turbo_alpaca_lora'\n",
    "config = PeftConfig.from_pretrained(model_name)\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b693c",
   "metadata": {},
   "source": [
    "## gusev_saiga_7b_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42712c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'IlyaGusev/saiga_7b_lora'\n",
    "config = PeftConfig.from_pretrained(model_name)\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f7fcc",
   "metadata": {},
   "source": [
    "## gusev_ru_alpaca_7b_llamacpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = Path.home()\n",
    "main_path = home_dir / 'src/llama.cpp/main'\n",
    "model_path = home_dir / 'models/llama_7b_ru_turbo_alpaca_lora_llamacpp/7B/ggml-model-q4_0.bin'\n",
    "generation_args = [\n",
    "    '--top_k', '40',\n",
    "    '--top_p', '0.9',\n",
    "    '--repeat_penalty', '1.1',\n",
    "    '--ctx_size', '256',\n",
    "    '--n_predict', '512',\n",
    "    '--temp', '0.95'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb88bb",
   "metadata": {},
   "source": [
    "## gusev_alpaca_7b_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50098bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'IlyaGusev/alpaca_7b_lora_reproduce'\n",
    "\n",
    "config = PeftConfig.from_pretrained(model_name)\n",
    "config.base_model_name_or_path = 'decapoda-research/llama-7b-hf'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b8dde",
   "metadata": {},
   "source": [
    "## chainyo_alpaca_7b_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd4f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'chainyo/alpaca-lora-7b'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.2,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb55b23b",
   "metadata": {},
   "source": [
    "## wortega_instruct_rugpt_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "039def6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'AlexWortega/instruct_rugptlarge'\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': [\n",
    "        '<code>', '</code>',\n",
    "        '<instructionS>', '<instructionE>',\n",
    "        '<next>'\n",
    "    ]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    min_length=20,\n",
    "    max_new_tokens=512,\n",
    "    top_k=50,\n",
    "    top_p=0.7,\n",
    "    do_sample=True,  \n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=2,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    repetition_penalty=1.5,  \n",
    "    length_penalty=1.2,  \n",
    "    num_beams=4,\n",
    ")\n",
    "\n",
    "device = 'cuda:0'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device).eval()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06652557",
   "metadata": {},
   "source": [
    "# evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4402cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_path = Path('tasks/vicuna_question_ru.jsonl')\n",
    "eval_path = Path('evals/vicuna_question_ru/gusev_ru_alpaca_13b_lora_v2.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44a735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_records = load_task(task_path)\n",
    "# eval_records = [\n",
    "#     EvalRecord(\n",
    "#         id=_.id,\n",
    "#         prompt=gusev_ru_alpaca_prompt(_),\n",
    "#         output=None\n",
    "#     )\n",
    "#     for _ in task_records\n",
    "# ]\n",
    "# random.sample(eval_records, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a89e7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_eval(eval_path, eval_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cbeeaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "eval_records = list(load_eval(eval_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c56a1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 80\n"
     ]
    }
   ],
   "source": [
    "total = len(eval_records)\n",
    "have_output = sum(\n",
    "    _.output is not None\n",
    "    for _ in eval_records\n",
    ")\n",
    "print(have_output, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba9f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -n main.py\n",
    "# model_batch_complete(eval_records, model, tokenizer, generation_config, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f6eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -n main.py\n",
    "# await batch_llamacpp_complete(\n",
    "#     eval_records, main_path, model_path, generation_args,\n",
    "#     threads=8, pool_size=3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20412d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_eval(eval_path, eval_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727d70c",
   "metadata": {},
   "source": [
    "# annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c637cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls evals/ru_vicuna_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "a_name = 'gusev_7b_ru_alpaca_lora'\n",
    "b_name = 'gusev_13b_ru_alpaca_lora'\n",
    "\n",
    "lines = read_lines(f'evals/ru_vicuna_question/{a_name}.jsonl')\n",
    "a_items = list(parse_jsonl(lines))\n",
    "\n",
    "lines = read_lines(f'evals/ru_vicuna_question/{b_name}.jsonl')\n",
    "b_items = list(parse_jsonl(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "id_a_items = {_['id']: _ for _ in a_items}\n",
    "id_b_items = {_['id']: _ for _ in b_items}\n",
    "items = []\n",
    "for id in sorted(id_a_items.keys() & id_b_items.keys()):\n",
    "    a_item = id_a_items[id]\n",
    "    b_item = id_b_items[id]\n",
    "\n",
    "    items.append({\n",
    "        'id': id,\n",
    "        'prompt': b_item['prompt'],\n",
    "        'a': a_item['output'],\n",
    "        'b': b_item['output'],\n",
    "        'label': None\n",
    "    })\n",
    "\n",
    "table = pd.DataFrame(items)\n",
    "table.to_excel(f'sbs/ru_vicuna_question/{a_name}_{b_name}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!open sbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6761ed",
   "metadata": {},
   "source": [
    "# report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddf8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a58e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "names = [\n",
    "#     'openai_turbo',\n",
    "    'openai_davinci_002',\n",
    "    'gusev_7b_ru_alpaca_lora',\n",
    "#     'gusev_13b_ru_alpaca_lora',\n",
    "    'wortega_instruct_rugpt_large',\n",
    "]\n",
    "\n",
    "\n",
    "data = {}\n",
    "for name in names:\n",
    "    path = f'sbs/ru_vicuna_question/openai_davinci_003_{name}.xlsx'\n",
    "    table = pd.read_excel(path, dtype='str')\n",
    "    \n",
    "    label_counts = table.label.value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        data[name, label] = count\n",
    "        \n",
    "# data['openai_davinci_003', '0'] = 30\n",
    "\n",
    "table = pd.Series(data)\n",
    "table = table.unstack()\n",
    "table = table.fillna(0)\n",
    "\n",
    "\n",
    "table = table.reindex(\n",
    "    index=[\n",
    "        'wortega_instruct_rugpt_large',\n",
    "\n",
    "#         'gusev_13b_ru_alpaca_lora',\n",
    "        'openai_davinci_002',\n",
    "        'gusev_7b_ru_alpaca_lora',\n",
    "\n",
    "#         'openai_turbo',\n",
    "        \n",
    "    ],\n",
    "    columns=[\n",
    "        '2', '1', '0', '-1', '-2', '?',\n",
    "    ]\n",
    "    \n",
    ")\n",
    "table = table.rename(\n",
    "    columns = {\n",
    "        '?': 'пустой ответ',\n",
    "        '-2': 'хуже',\n",
    "        '-1': 'похуже',\n",
    "        '0': 'примерно одинаково',\n",
    "        '1': 'получше',\n",
    "        '2': 'лучше'\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "table.plot(\n",
    "    title='SbS с openai_davinci_003 на ru_vicuna_question',\n",
    "    kind='barh',\n",
    "    stacked=True,\n",
    "    width=0.9,\n",
    "    alpha=0.7,\n",
    "    xlabel='# заданий'\n",
    ").legend(\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.0, 1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!open sbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ca193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
