{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f1cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "lines = read_lines('.secret')\n",
    "env = dict(parse_dotenv(lines))\n",
    "openai.api_key = env['OPENAI_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91801c88",
   "metadata": {},
   "source": [
    "# tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = pd.read_excel('tasks/user_oriented_annot.xlsx')\n",
    "# table = table[~table.done.isnull()]\n",
    "# table = table.where(pd.notnull(table), None)\n",
    "\n",
    "# view = table[['id', 'orig_instruction', 'orig_input']]\n",
    "# view = view.rename(columns={\n",
    "#     'orig_instruction': 'instruction',\n",
    "#     'orig_input': 'input'\n",
    "# })\n",
    "# items = view.to_dict(orient='records')\n",
    "# lines = format_jsonl(items)\n",
    "# write_lines('tasks/user_oriented_en.jsonl', lines)\n",
    "\n",
    "# view = table[['id', 'instruction', 'input']]\n",
    "# items = view.to_dict(orient='records')\n",
    "# lines = format_jsonl(items)\n",
    "# write_lines('tasks/user_oriented_ru.jsonl', lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = pd.read_excel('tasks/vicuna_question_annot.xlsx')\n",
    "\n",
    "# view = table[['id', 'category', 'instruction']]\n",
    "# items = view.to_dict(orient='records')\n",
    "# lines = format_jsonl(items)\n",
    "# write_lines('tasks/vicuna_question_ru.jsonl', lines)\n",
    "\n",
    "# view = table[['id', 'category', 'orig_instruction']]\n",
    "# view = view.rename(columns={'orig_instruction': 'instruction'})\n",
    "# items = view.to_dict(orient='records')\n",
    "# lines = format_jsonl(items)\n",
    "# write_lines('tasks/vicuna_question_en.jsonl', lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277a70a",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75304a05",
   "metadata": {},
   "source": [
    "## gusev_7b_ru_alpaca_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM.int8() requires Turing or Ampere GPUs.\n",
    "# WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-11.7/targets/x86_64-linux/lib'\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822527ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = 'IlyaGusev/llama_7b_ru_turbo_alpaca_lora'\n",
    "config = PeftConfig.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "\n",
    "    # Overriding torch_dtype=None with `torch_dtype=torch.float16\n",
    "    torch_dtype=torch.float16,\n",
    "    \n",
    "    # A device map needs to be passed to run convert models into mixed-int8\n",
    "    device_map='auto'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = PeftModel.from_pretrained(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d3866",
   "metadata": {},
   "source": [
    "## gusev_13b_ru_alpaca_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'IlyaGusev/llama_13b_ru_turbo_alpaca_lora'\n",
    "config = PeftConfig.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = PeftModel.from_pretrained(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb88bb",
   "metadata": {},
   "source": [
    "## gusev_7b_en_alpaca_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50098bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'IlyaGusev/alpaca_7b_lora_reproduce'\n",
    "\n",
    "config = PeftConfig.from_pretrained(model_name)\n",
    "config.base_model_name_or_path = 'decapoda-research/llama-7b-hf'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = PeftModel.from_pretrained(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b8dde",
   "metadata": {},
   "source": [
    "## chainyo_7b_en_alpaca_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd4f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'chainyo/alpaca-lora-7b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb55b23b",
   "metadata": {},
   "source": [
    "## wortega_instruct_rugpt_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039def6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    GPT2LMHeadModel\n",
    ")\n",
    "\n",
    "model_name = 'AlexWortega/instruct_rugptlarge'\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': [\n",
    "        '<code>', '</code>',\n",
    "        '<instructionS>', '<instructionE>',\n",
    "        '<next>'\n",
    "    ]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "device = 'cuda:0'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1216ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in log_progress(task_records):\n",
    "    prompt = instruct_rugpt_prompt(record)\n",
    "    output = instruct_rugpt_complete(prompt, model, tokenizer)\n",
    "    eval_records.append(EvalRecord(record.id, prompt, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06652557",
   "metadata": {},
   "source": [
    "# evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134cead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "task_path = 'tasks/user_oriented_ru.jsonl'\n",
    "eval_path = 'evals/user_oriented_ru/openai_turbo.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbeeaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_records = list(load_eval(eval_path))\n",
    "cache_ids = {_.id for _ in eval_records}\n",
    "task_records = [\n",
    "    _ for _ in load_task(task_path)\n",
    "    if _.id not in cache_ids\n",
    "]\n",
    "print('eval:', len(eval_records))\n",
    "print('task:', len(task_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba9f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "for task_record in log_progress(task_records):\n",
    "    eval_record = eval_ru_openai(task_record, GPT_35_TURBO)\n",
    "\n",
    "    print(eval_record)\n",
    "    if eval_record:\n",
    "        eval_records.append(eval_record) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20412d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_eval(eval_path, eval_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727d70c",
   "metadata": {},
   "source": [
    "# annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c637cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls evals/ru_vicuna_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "a_name = 'gusev_7b_ru_alpaca_lora'\n",
    "b_name = 'gusev_13b_ru_alpaca_lora'\n",
    "\n",
    "lines = read_lines(f'evals/ru_vicuna_question/{a_name}.jsonl')\n",
    "a_items = list(parse_jsonl(lines))\n",
    "\n",
    "lines = read_lines(f'evals/ru_vicuna_question/{b_name}.jsonl')\n",
    "b_items = list(parse_jsonl(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "id_a_items = {_['id']: _ for _ in a_items}\n",
    "id_b_items = {_['id']: _ for _ in b_items}\n",
    "items = []\n",
    "for id in sorted(id_a_items.keys() & id_b_items.keys()):\n",
    "    a_item = id_a_items[id]\n",
    "    b_item = id_b_items[id]\n",
    "\n",
    "    items.append({\n",
    "        'id': id,\n",
    "        'prompt': b_item['prompt'],\n",
    "        'a': a_item['output'],\n",
    "        'b': b_item['output'],\n",
    "        'label': None\n",
    "    })\n",
    "\n",
    "table = pd.DataFrame(items)\n",
    "table.to_excel(f'sbs/ru_vicuna_question/{a_name}_{b_name}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!open sbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6761ed",
   "metadata": {},
   "source": [
    "# report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddf8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a58e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "names = [\n",
    "#     'openai_turbo',\n",
    "    'openai_davinci_002',\n",
    "    'gusev_7b_ru_alpaca_lora',\n",
    "#     'gusev_13b_ru_alpaca_lora',\n",
    "    'wortega_instruct_rugpt_large',\n",
    "]\n",
    "\n",
    "\n",
    "data = {}\n",
    "for name in names:\n",
    "    path = f'sbs/ru_vicuna_question/openai_davinci_003_{name}.xlsx'\n",
    "    table = pd.read_excel(path, dtype='str')\n",
    "    \n",
    "    label_counts = table.label.value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        data[name, label] = count\n",
    "        \n",
    "# data['openai_davinci_003', '0'] = 30\n",
    "\n",
    "table = pd.Series(data)\n",
    "table = table.unstack()\n",
    "table = table.fillna(0)\n",
    "\n",
    "\n",
    "table = table.reindex(\n",
    "    index=[\n",
    "        'wortega_instruct_rugpt_large',\n",
    "\n",
    "#         'gusev_13b_ru_alpaca_lora',\n",
    "        'openai_davinci_002',\n",
    "        'gusev_7b_ru_alpaca_lora',\n",
    "\n",
    "#         'openai_turbo',\n",
    "        \n",
    "    ],\n",
    "    columns=[\n",
    "        '2', '1', '0', '-1', '-2', '?',\n",
    "    ]\n",
    "    \n",
    ")\n",
    "table = table.rename(\n",
    "    columns = {\n",
    "        '?': 'пустой ответ',\n",
    "        '-2': 'хуже',\n",
    "        '-1': 'похуже',\n",
    "        '0': 'примерно одинаково',\n",
    "        '1': 'получше',\n",
    "        '2': 'лучше'\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "table.plot(\n",
    "    title='SbS с openai_davinci_003 на ru_vicuna_question',\n",
    "    kind='barh',\n",
    "    stacked=True,\n",
    "    width=0.9,\n",
    "    alpha=0.7,\n",
    "    xlabel='# заданий'\n",
    ").legend(\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.0, 1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!open sbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ca193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
