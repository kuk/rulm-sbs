{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f1cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = read_lines(DOTENV_PATH)\n",
    "# env = dict(parse_dotenv(lines))\n",
    "# openai.api_key = env['OPENAI_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91801c88",
   "metadata": {},
   "source": [
    "# tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv('tasks/ru_user_oriented.csv')\n",
    "table = table[~table.done.isnull()]\n",
    "table = table.where(pd.notnull(table), None)\n",
    "table = table[['id', 'instruction', 'input']]\n",
    "ru_user_oriented_items = table.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277a70a",
   "metadata": {},
   "source": [
    "# evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75304a05",
   "metadata": {},
   "source": [
    "## llama_7b_ru_turbo_alpaca_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822527ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LLM.int8() requires Turing or Ampere GPUs.\n",
    "# WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-11.7/targets/x86_64-linux/lib'\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_name = 'IlyaGusev/llama_7b_ru_turbo_alpaca_lora'\n",
    "\n",
    "config = PeftConfig.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "\n",
    "    # Overriding torch_dtype=None with `torch_dtype=torch.float16\n",
    "    torch_dtype=torch.float16,\n",
    "    \n",
    "    # A device map needs to be passed to run convert models into mixed-int8\n",
    "    device_map='auto'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = PeftModel.from_pretrained(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c345f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "lines = read_lines('evals/gusev_7b_ru_alpaca_lora.jsonl')\n",
    "eval_items = list(parse_jsonl(lines))\n",
    "cache_ids = {_['id'] for _ in eval_items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ccaaa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "items = [\n",
    "    _ for _ in ru_user_oriented_items\n",
    "    if _['id'] not in cache_ids\n",
    "]\n",
    "for item in log_progress(items):\n",
    "    prompt = user_oriented_ru_alcapa_prompt(item)\n",
    "    output = ru_alpaca_complete(prompt, model, tokenizer)\n",
    "    item = {\n",
    "        'id': item['id'],\n",
    "        'prompt': prompt,\n",
    "        'output': output\n",
    "    }\n",
    "    eval_items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24650c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "lines = format_jsonl(eval_items)\n",
    "write_lines('evals/gusev_7b_ru_alpaca_lora.jsonl', lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d3866",
   "metadata": {},
   "source": [
    "## llama_13b_ru_turbo_alpaca_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-11.7/targets/x86_64-linux/lib'\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_name = 'IlyaGusev/llama_13b_ru_turbo_alpaca_lora'\n",
    "\n",
    "config = PeftConfig.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = PeftModel.from_pretrained(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c88507",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "lines = read_lines('evals/gusev_13b_ru_alpaca_lora.jsonl')\n",
    "eval_items = list(parse_jsonl(lines))\n",
    "cache_ids = {_['id'] for _ in eval_items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826eab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\n",
    "    _ for _ in ru_user_oriented_items\n",
    "    if _['id'] not in cache_ids\n",
    "]\n",
    "for item in log_progress(items):\n",
    "    prompt = user_oriented_ru_alcapa_prompt(item)\n",
    "    output = ru_alpaca_complete(prompt, model, tokenizer)\n",
    "    item = {\n",
    "        'id': item['id'],\n",
    "        'prompt': prompt,\n",
    "        'output': output\n",
    "    }\n",
    "    eval_items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ad313",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "lines = format_jsonl(eval_items)\n",
    "write_lines('evals/gusev_13b_ru_alpaca_lora.jsonl', lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb55b23b",
   "metadata": {},
   "source": [
    "## instruct_rugptlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039def6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "model_name = 'AlexWortega/instruct_rugptlarge'\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': [\n",
    "        '<code>', '</code>',\n",
    "        '<instructionS>', '<instructionE>',\n",
    "        '<next>'\n",
    "    ]\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe53336",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "lines = read_lines('evals/wortega_instruct_rugpt_large.jsonl')\n",
    "eval_items = list(parse_jsonl(lines))\n",
    "cache_ids = {_['id'] for _ in eval_items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1216ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\n",
    "    _ for _ in ru_user_oriented_items\n",
    "    if _['id'] not in cache_ids\n",
    "]\n",
    "for item in log_progress(items):\n",
    "    prompt = user_oriented_instruct_rugpt(item)\n",
    "    output = instruct_rugpt_complete(prompt, model, tokenizer)\n",
    "    item = {\n",
    "        'id': item['id'],\n",
    "        'prompt': prompt,\n",
    "        'output': output\n",
    "    }\n",
    "    eval_items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "lines = format_jsonl(eval_items)\n",
    "write_lines('evals/wortega_instruct_rugpt_large.jsonl', lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6200c",
   "metadata": {},
   "source": [
    "## gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbf0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_lines('evals/openai_turbo.jsonl')\n",
    "eval_items = list(parse_jsonl(lines))\n",
    "cache_ids = {_['id'] for _ in eval_items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b711c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\n",
    "    _ for _ in ru_user_oriented_items\n",
    "    if _['id'] not in cache_ids\n",
    "]\n",
    "for item in log_progress(items):\n",
    "    prompt = user_oriented_openai_prompt(item)\n",
    "    output = openai_chat_complete(prompt, model='gpt-3.5-turbo')\n",
    "    item = {\n",
    "        'id': item['id'],\n",
    "        'prompt': prompt,\n",
    "        'output': output\n",
    "    }\n",
    "    eval_items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "lines = format_jsonl(eval_items)\n",
    "write_lines('evals/openai_turbo.jsonl', lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae752e",
   "metadata": {},
   "source": [
    "## text-davinci-003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34739f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_lines('evals/openai_davinci_003.jsonl')\n",
    "eval_items = list(parse_jsonl(lines))\n",
    "cache_ids = {_['id'] for _ in eval_items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f6519",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "items = [\n",
    "    _ for _ in ru_user_oriented_items\n",
    "    if _['id'] not in cache_ids\n",
    "]\n",
    "for item in log_progress(items):\n",
    "    prompt = user_oriented_openai_prompt(item)\n",
    "    output = openai_complete(prompt, model='text-davinci-003')\n",
    "    item = {\n",
    "        'id': item['id'],\n",
    "        'prompt': prompt,\n",
    "        'output': output\n",
    "    }\n",
    "    eval_items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "lines = format_jsonl(eval_items)\n",
    "write_lines('evals/openai_davinci_003.jsonl', lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d1883",
   "metadata": {},
   "source": [
    "## text-davinci-002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_lines('evals/openai_davinci_002.jsonl')\n",
    "eval_items = list(parse_jsonl(lines))\n",
    "cache_ids = {_['id'] for _ in eval_items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\n",
    "    _ for _ in ru_user_oriented_items\n",
    "    if _['id'] not in cache_ids\n",
    "]\n",
    "for item in log_progress(items):\n",
    "    prompt = user_oriented_openai_prompt(item)\n",
    "    output = openai_complete(prompt, model='text-davinci-002')\n",
    "    item = {\n",
    "        'id': item['id'],\n",
    "        'prompt': prompt,\n",
    "        'output': output\n",
    "    }\n",
    "    eval_items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b28fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "lines = format_jsonl(eval_items)\n",
    "write_lines('evals/openai_davinci_002.jsonl', lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb1088b",
   "metadata": {},
   "source": [
    "## davinci-001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b70559",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_lines('evals/openai_davinci_001.jsonl')\n",
    "eval_items = list(parse_jsonl(lines))\n",
    "cache_ids = {_['id'] for _ in eval_items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\n",
    "    _ for _ in ru_user_oriented_items\n",
    "    if _['id'] not in cache_ids\n",
    "]\n",
    "for item in log_progress(items):\n",
    "    prompt = user_oriented_openai_prompt(item)\n",
    "    output = openai_complete(prompt, model='davinci')\n",
    "    item = {\n",
    "        'id': item['id'],\n",
    "        'prompt': prompt,\n",
    "        'output': output\n",
    "    }\n",
    "    eval_items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "lines = format_jsonl(eval_items)\n",
    "write_lines('evals/openai_davinci_001.jsonl', lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3627a717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
